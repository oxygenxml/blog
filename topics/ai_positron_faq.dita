<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="ai_positron_faq">
  <title>AI Positron Troubleshooting and FAQ</title>
  <shortdesc>Common questions and troubleshooting guidance for AI Positron users, covering LLM selection, configuration, error messages, and best practices.</shortdesc>
  <prolog>
    <author>Radu Coravu</author>
    <critdates>
      <created date="2026-02-18"/>
    </critdates>
  </prolog>
  <body>
    <section id="overview">
      <title>Overview</title>
      <p>This FAQ addresses common issues and questions encountered when using AI Positron with
        Oxygen desktop, Oxygen Content Fusion CMS, or integrated in Web Author with a third-party
        CMS. The guidance provided is based on real-world user experiences and technical support
        interactions.</p>
    </section>

    <section id="llm_selection">
      <title>LLM Selection and Configuration</title>
      
      <p><b>Q: What LLM models are recommended for AI Positron?</b></p>
      <p>A: AI Positron works best with capable LLMs such as:</p>
      <ul>
        <li>Claude Sonnet</li>
        <li>Claude Haiku</li>
        <li>OpenAI GPT 5.1</li>
        <li>OpenAI GPT 4.1</li>
      </ul>
      <p>If you use a less capable LLM, AI Positron may not produce optimal results. The quality of responses is directly dependent on the LLM's capabilities.</p>

      <p><b>Q: Why is my LLM choice important?</b></p>
      <p>A: The LLM model is the most critical factor in AI Positron's performance. Many issues
        reported (poor response quality, hallucinations, slow performance) are related to LLM
        limitations rather than AI Positron itself. Ensure that you're using a capable model.</p>

      <p><b>Q: What is a project context prompt and why do I need one?</b></p>
      <p>A: A project context prompt is a set of instructions that describes your project's characteristics, terminology, style guidelines, and specific requirements. It helps the LLM generate more accurate and relevant responses tailored to your documentation standards. For detailed guidance, see <xref href="customizing_ai_positron_for_your_dita_xml_project.dita"/>.</p>
    </section>

    <section id="token_errors">
      <title>Token and Context Window Errors</title>

      <p><b>Q: What does "Exceeded … allowed tokens …" error mean?</b></p>
      <p>A: This error indicates that the LLM's context window is not large enough to process your file. Modern LLMs typically have about 200k tokens of context window and can process files up to approximately 40 kilobytes without issues. If you encounter this error:</p>
      <ul>
        <li>Break your content into smaller files.</li>
        <li>Use an LLM with a larger context window.</li>
        <li>Reduce the amount of content sent to the LLM in a single request.</li>
      </ul>
      <p>This is a limitation of the LLM itself, not something AI Positron can fix.</p>

      <p><b>Q: How can I avoid token limit errors?</b></p>
      <p>A: Keep individual topics reasonably sized (under 40KB). If you have very large topics, consider breaking them into smaller, more focused topics following DITA best practices.</p>
    </section>

    <section id="image_errors">
      <title>Reporting problems</title>

      <p><b>Q: What should I do when I want to report a problem based on an interaction in the Chat
          view?</b></p>
      <p>A: Export your chat conversation (for AI Positron Desktop, use the export action in the
        "Actions" drop-down menu; for AI Positron Web Author, type "/" in the chat input and select
        "Export chat") and contact support with the exported JSON file.</p>
    </section>

    <section id="mathml_latex">
      <title>MathML and LaTeX Issues</title>

      <p><b>Q: Why does AI Positron generate invalid MathML from LaTeX?</b></p>
      <p>A: This depends on the LLM model's capabilities. To improve results, add examples of correct LaTeX to MathML conversions in your project context prompt. This helps guide the LLM toward the correct format.</p>
    </section>

    <section id="checkout_issues">
      <title>Topic Checkout and Editing Issues</title>

      <p><b>Q: Why do I get errors when using AI Positron without checking out the topic?</b></p>
      <p>A: When Positron is used in a CMS integration and a topic is not checked out, the add-on
        cannot make changes to the opened document. Always check out your topic before using AI
        Positron features.</p>

      <p><b>Q: Why don't my changes appear until I refresh the browser?</b></p>
      <p>A: In some scenarios of third-party CMS integrations, AI Positron edits the topic source
        but changes remain hidden until you refresh the browser. This is an integration issue
        between the CMS and AI Positron. Open an issue with the CMS requesting better integration
        with AI Positron.</p>
    </section>

    <section id="preview_compare">
      <title>Preview and Compare Tool Issues</title>

      <p><b>Q: Why does the Compare tool show LLM reasoning instead of topic changes?</b></p>
      <p>A: The LLM may output reasoning along with the modified content. To fix this, instruct the LLM in your project context prompt to wrap modified content in Markdown codeblocks when providing explanations:</p>
      <codeblock>When replying with both explanations and modified content, ALWAYS wrap the modified content in Markdown codeblocks.</codeblock>
      <p>Once the LLM returns content in Markdown code blocks, AI Positron will correctly identify and use only the modified content.</p>

      <p><b>Q: Why do content references appear unresolved in the Compare tool?</b></p>
      <p>A: Keyrefs and conkeyrefs are not being resolved in the preview. This is sometimes a CMS
        integration issue. Ask the CMS to contact the Oxygen team about improving this
        functionality.</p>
    </section>

    <section id="response_insertion">
      <title>Response Insertion and Content Issues</title>

      <p><b>Q: Why does "Insert response at caret position" insert LLM commentary instead of the
          requested content?</b></p>
      <p>A: The LLM is providing both explanations and modified content without clear separation. Update your project context prompt to instruct the LLM:</p>
      <codeblock>When replying with both explanations and modified content, ALWAYS wrap the modified content in Markdown codeblocks.</codeblock>
      <p>This helps AI Positron distinguish between the actual content and explanatory text.</p>

      <p><b>Q: Why does "Suggest improved title" insert the entire response instead of just the title?</b></p>
      <p>A: Same issue as above. The LLM needs to wrap the suggested title in Markdown codeblocks if it also wants to provide discussion about the suggestion.</p>

      <p><b>Q: Why does Positron insert reasoning into the topic source?</b></p>
      <p>A: The LLM is responding with both explanations and modified content without clear separation. Use the Markdown codeblock approach described above to resolve this.</p>
    </section>

    <section id="ai_actions">
      <title>AI Action Issues</title>

      <p><b>Q: Why does the "Correct Grammar" button report errors in metadata elements?</b></p>
      <p>A: The Correct Grammar action checks the underlying XML metadata along with the text content. This depends on the LLM model quality. You can create a custom AI action with a more refined prompt. Contact support for the definition of the Correct Grammar action if you want to customize it.</p>

      <p><b>Q: Why does "Join Items" create verbose responses?</b></p>
      <p>A: This depends on the LLM model. You can create your own custom Join Items AI action with a more concise prompt. Contact support for the action definition.</p>

      <p><b>Q: Why does "Readability" provide vague suggestions?</b></p>
      <p>A: The default Readability action may be too generic for your needs. </p>
      <p>Contact support for the Readability action definition and create a custom version tailored to your project's needs.</p>

      <p><b>Q: Why does "Use Active Voice" include false positives?</b></p>
      <p>A: The action may flag metadata elements and attributes incorrectly. This depends on the LLM model quality. You can create a custom AI action with a more refined prompt. Contact support for the action definition.</p>

      <p><b>Q: Why does "Generate Alt Text" produce descriptions that are too brief?</b></p>
      <p>A: After invoking the Generate Alt Text action, continue the conversation in the chat. Ask the LLM to expand the description or make it more detailed. Alternatively, create a custom AI action with a prompt that emphasizes detail and specificity. Contact support for the action definition.</p>

      <p><b>Q: Why does "Formula/Equation" button fail?</b></p>
      <p>A: This may depend on the LLM model quality. You can create your own custom AI action for formula generation. Contact support for the default action definition if you want to customize it.</p>
    </section>

    <section id="ui_usability">
      <title>UI and Usability Issues</title>

      <p><b>Q: Why don't my Favorites persist across sessions?</b></p>
      <p>A: In certain CMS AI Positron Web Author integrations, favorites only persist within the
        current topic's session. To persist them across sessions, the CMS needs to implement APIs on
        their side. Open an issue with the CMS requesting this feature.</p>

      <p><b>Q: Why doesn't the History drop-down persist?</b></p>
      <p>A: Similar to Favorites, the History dropdown requires CMS-side APIs to persist data.</p>

      <p><b>Q: Why can't I upload files of type X?</b></p>
      <p>A: File upload support is limited to a list of types mentioned in the documentation
        (including XML, text, Markdown, Word). Provide details about the specific error message when
        reporting this issue, and note whether the error occurs consistently or intermittently.</p>
    </section>

    <section id="performance">
      <title>Performance Issues</title>

      <p><b>Q: Why is performance lackluster in some scenarios?</b></p>
      <p>A: Performance depends primarily on the LLM model you're using. AI Positron cannot improve performance beyond what the LLM provides. Consider using a faster or more efficient LLM model.</p>

      <p><b>Q: Why does analyzing a topic take a long time?</b></p>
      <p>A: This slowdown is related to the LLM model's performance. Try using a faster LLM model or breaking your analysis into smaller chunks.</p>
    </section>

    <section id="dtd_validation">
      <title>DTD and Structural Validation Issues</title>

      <p><b>Q: Why does AI Positron generate content that violates my DTD?</b></p>
      <p>A: This depends on the LLM model quality. To improve results:</p>
      <ul>
        <li>Provide examples of correct structures in your project context prompt.</li>
        <li>Instruct the LLM to avoid specific mistakes it frequently makes.</li>
        <li>Create custom AI actions with more specific prompts.</li>
      </ul>
    </section>

    <section id="content_security">
      <title>Content Security and Hallucinations</title>

      <p><b>Q: Can AI Positron retrieve information from outside my network?</b></p>
      <p>A: No. AI Positron does not have tools to retrieve content from outside your network. If responses reference external sources or competitor information, these are LLM hallucinations. To mitigate this:</p>
      <ul>
        <li>Instruct the LLM to avoid certain types of responses in your project context
          prompt.</li>
        <li>When using AI Positron Web Author with a third-party CMS, ask the CMS to implement tools
          that give the LLM access to keyword search your project, enabling Retrieval Augmented
          Generation (RAG).</li>
        <li>Consider implementing an LLM server proxy with RAG capabilities.</li>
      </ul>

      <p><b>Q: How can I prevent AI Positron from discussing competitor software?</b></p>
      <p>A: Add instructions to your project context prompt explicitly telling the LLM to avoid discussing competitor products or external sources. This helps reduce hallucinations and IP/copyright risks.</p>
    </section>

    <section id="best_practices">
      <title>Best Practices and Recommendations</title>

      <p><b>Always upgrade to the latest AI Positron version</b></p>
      <p>The latest version includes improvements and bug fixes. See <xref
          href="https://www.oxygenxml.com/ai_positron/whats_new.html" format="html" scope="external"
        /> for details.</p>

      <p><b>Create a comprehensive project context prompt</b></p>
      <p>This is the second most important factor after LLM selection. Your project context prompt should include:</p>
      <ul>
        <li>Project characteristics and goals.</li>
        <li>Terminology and style guidelines.</li>
        <li>Examples of correct structures and formats.</li>
        <li>Instructions to avoid common mistakes.</li>
        <li>Guidance on wrapping modified content in Markdown codeblocks.</li>
        <li>Instructions to avoid discussing external sources or competitors.</li>
      </ul>
      <p>See <xref href="customizing_ai_positron_for_your_dita_xml_project.dita"/> for detailed guidance.</p>

      <p><b>Use AI actions instead of manual Chat prompts</b></p>
      <p>The dedicated AI action buttons (Correct Grammar, Readability, etc.) work better than
        manual prompts in the Chat window, especially with lower quality LLMs.</p>

      <p><b>Export chat conversations for support</b></p>
      <p>When reporting issues, export your chat conversation. This helps support diagnose
        problems.</p>

      <p><b>Provide detailed reproduction steps</b></p>
      <p>When reporting issues, include:</p>
      <ul>
        <li>A sample topic that reproduces the problem.</li>
        <li>Step-by-step instructions to reproduce the issue.</li>
        <li>Screenshots if applicable.</li>
        <li>Your LLM model and version information.</li>
      </ul>

      <p><b>Consider alternative platforms for better Chat support</b></p>
      <p>If you need robust Chat functionality with RAG support, consider using Oxygen desktop or
        Oxygen Content Fusion CMS instead of integrations with a third-party CMS. See <xref
          href="vibing_with_ai_positron.dita"/> for more information.</p>
    </section>

    <section id="getting_help">
      <title>Getting Help</title>

      <p>If you encounter issues not covered in this FAQ:</p>
      <ul>
        <li>Check the <xref href="customizing_ai_positron_for_your_dita_xml_project.dita"/> article
          for configuration guidance.</li>
        <li>Review <xref href="vibing_with_ai_positron.dita"/> for best practices on using AI
          Positron effectively.</li>
        <li>Export your chat conversation and submit it via the <xref
            href="https://www.oxygenxml.com/techSupport.html" format="html" scope="external">tech
            support form</xref>.</li>
        <li>For issues that need to be implemented by a better CMS integration with AI Positron,
          please report them to the CMS owners.</li>
      </ul>
    </section>
  </body>
</topic>
